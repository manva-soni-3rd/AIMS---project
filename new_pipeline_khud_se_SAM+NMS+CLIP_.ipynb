{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVaOt2kcnmFadDHEeLler0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manva-soni-3rd/AIMS---project/blob/main/new_pipeline_khud_se_SAM%2BNMS%2BCLIP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Part 1: Setup and Installation (Run Once Per Session)\n",
        "# ===================================================================\n",
        "print(\" PART 1: INSTALLING LIBRARIES & DOWNLOADING MODELS \")\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q transformers\n",
        "\n",
        "# Download the SMALLER 'vit_b' SAM model checkpoint file for memory efficiency\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "print(\"\\n All libraries and the SAM model downloaded.\")\n",
        "print(\"You can now proceed to the next cell to load the models.\")\n"
      ],
      "metadata": {
        "id": "dQjaK8Ut6b6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using a pre-trained - SAM and CLIP model + Non-max supression."
      ],
      "metadata": {
        "id": "ganyIQuIBJDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Part 2: Load Models into Memory (Run Once Per Session)\n",
        "# ===================================================================\n",
        "print(\"\\n PART 2: LOADING MODELS ONTO GPU \")\n",
        "\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "from torchvision.ops import nms\n",
        "\n",
        "# Setup device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Segment Anything Model (SAM) ---\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "# Use memory-efficient settings for the mask generator\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=16,\n",
        "    crop_n_layers=1,\n",
        "    crop_n_points_downscale_factor=2\n",
        ")\n",
        "print(\" SAM model loaded.\")\n",
        "\n",
        "# loading clip\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "language_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "language_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "print(\" CLIP model loaded.\")\n",
        "print(\"\\nModels are ready. You can now run the inference cell.\")\n"
      ],
      "metadata": {
        "id": "fmvmQ3nR6ePl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In part 3 - aka implementation we are working in a 5 step process -\n",
        "1. loading models and libraries - and text queries and image.\n",
        "2. changing the size of the image to deal with the Cuda memory error - note if the resultion is high - the better the results.\n",
        "3. hierarchy model - in simple terms - this takes in account all the segmented parts, and checks which mask is part of a bigger parent mask according to the query given.\n",
        "4. The clip then assigns the score to different masks the sam proposes - and gives the best match to the query.\n",
        "5. finally somewhat of a gate - that allows only a answer to pass - that matches or exceeds the threshold proposed by it. this ensures that all the negative cases do no return anything."
      ],
      "metadata": {
        "id": "AE3p02dY-QYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Part 3: Run the Full Pipeline with Hierarchical Analysis\n",
        "# ===================================================================\n",
        "print(\"\\nPART 3: RUNNING THE PIPELINE \")\n",
        "\n",
        "# --- Install supervision library for drawing annotations ---\n",
        "!pip install -q supervision\n",
        "\n",
        "import supervision as sv\n",
        "\n",
        "#  1. Define Image and Text Query ---\n",
        "image_url = 'https://i.ytimg.com/vi/qMnw30lgRNw/maxresdefault.jpg' # Person on a bike\n",
        "text_query = \"mickey mouse\" # A query that might focus on the head\n",
        "print(f\"Processing image: {image_url}\")\n",
        "print(f\"Searching for: '{text_query}'\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(image_url)\n",
        "    image_pil = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    print(f\"\\n Image loaded successfully. Original size: {image_pil.size}\")\n",
        "\n",
        "    # resolution -\n",
        "    max_dimension = 2000\n",
        "    if max(image_pil.size) > max_dimension:\n",
        "        image_pil.thumbnail((max_dimension, max_dimension))\n",
        "        print(f\"Resized image to: {image_pil.size} for processing.\")\n",
        "\n",
        "    # Check if the loaded image has valid dimensions before proceeding\n",
        "    if image_pil.width > 0 and image_pil.height > 0:\n",
        "        image_np = np.array(image_pil)\n",
        "\n",
        "        #  2.SAM\n",
        "        print(\"ðŸ”Ž Finding all potential objects with SAM...\")\n",
        "        sam_results = mask_generator.generate(image_np)\n",
        "        print(f\"Found {len(sam_results)} potential object masks.\")\n",
        "\n",
        "        # 3.Hierarchy\n",
        "        print(\" Analyzing part-whole relationships...\")\n",
        "        proposals = []\n",
        "        total_image_area = image_np.shape[0] * image_np.shape[1]\n",
        "\n",
        "        for i, mask_data in enumerate(sam_results):\n",
        "            bbox_xywh = mask_data['bbox']\n",
        "            box_area = bbox_xywh[2] * bbox_xywh[3]\n",
        "            if box_area / total_image_area > 0.95:\n",
        "                continue\n",
        "\n",
        "            box_xyxy = [bbox_xywh[0], bbox_xywh[1], bbox_xywh[0] + bbox_xywh[2], bbox_xywh[1] + bbox_xywh[3]]\n",
        "            proposals.append({\n",
        "                \"box\": np.array(box_xyxy),\n",
        "                \"mask\": mask_data['segmentation'],\n",
        "                \"score\": mask_data['predicted_iou'],\n",
        "                \"parent\": None\n",
        "            })\n",
        "        print(f\"Kept {len(proposals)} proposals after filtering.\")\n",
        "\n",
        "        for i, proposal_i in enumerate(proposals):\n",
        "            for j, proposal_j in enumerate(proposals):\n",
        "                if i == j: continue\n",
        "                if (proposal_i[\"box\"][0] >= proposal_j[\"box\"][0] - 5 and\n",
        "                    proposal_i[\"box\"][1] >= proposal_j[\"box\"][1] - 5 and\n",
        "                    proposal_i[\"box\"][2] <= proposal_j[\"box\"][2] + 5 and\n",
        "                    proposal_i[\"box\"][3] <= proposal_j[\"box\"][3] + 5):\n",
        "                    if proposal_i[\"parent\"] is None or np.prod(proposals[proposal_i[\"parent\"]][\"box\"][2:] - proposals[proposal_i[\"parent\"]][\"box\"][:2]) > np.prod(proposal_j[\"box\"][2:] - proposal_j[\"box\"][:2]):\n",
        "                         proposal_i[\"parent\"] = j\n",
        "\n",
        "        # 4. CLIP\n",
        "        print(\" Scoring all proposals with CLIP...\")\n",
        "        if not proposals:\n",
        "             print(\"\\nNo initial proposals found by SAM.\")\n",
        "        else:\n",
        "            cropped_images = [image_pil.crop(p['box']) for p in proposals]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                text_inputs = language_processor(text=[text_query], return_tensors=\"pt\", padding=True).to(device)\n",
        "                text_features = language_model.get_text_features(**text_inputs)\n",
        "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                inputs_images = language_processor(images=cropped_images, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "                features_images = language_model.get_image_features(**inputs_images)\n",
        "                features_images /= features_images.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                similarity_scores = (text_features @ features_images.T).squeeze(0)\n",
        "\n",
        "            # 5. Confidence Gate\n",
        "            best_detection_idx = torch.argmax(similarity_scores).item()\n",
        "            best_score = similarity_scores[best_detection_idx].item()\n",
        "\n",
        "            final_confidence_threshold = 0.25 # You can tune this value\n",
        "\n",
        "            if best_score >= final_confidence_threshold:\n",
        "                final_proposal_idx = best_detection_idx\n",
        "\n",
        "                if proposals[best_detection_idx][\"parent\"] is not None:\n",
        "                    print(\"Best match was a part of a larger object. Selecting the parent object.\")\n",
        "                    final_proposal_idx = proposals[best_detection_idx][\"parent\"]\n",
        "\n",
        "                final_proposal = proposals[final_proposal_idx]\n",
        "                final_cropped_image = image_pil.crop(final_proposal[\"box\"])\n",
        "\n",
        "                print(f\"\\nâœ… Detection complete. Displaying the cropped region for '{text_query}':\")\n",
        "                print(f\"(Confidence Score: {best_score:.2f})\")\n",
        "                display(final_cropped_image)\n",
        "            else:\n",
        "                print(f\"\\n No confident match found for the query.\")\n",
        "                print(f\"(Highest confidence score was {best_score:.2f}, which is below the threshold of {final_confidence_threshold})\")\n",
        "\n",
        "    else:\n",
        "        print(f\" ERROR: Image from {image_url} is invalid or has zero dimensions.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "5dWW-l7A390i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}